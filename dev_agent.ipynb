{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d21beeba-204c-4bf2-94a9-2f43c35dabc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ollama\n",
    "import whisper\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51f915c8-9729-445f-a8e8-bae4ab233369",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_CHARS = ''.join(map(chr, range(0, 32)))\n",
    "CONTROL_CHAR_RE = re.compile('[%s]' % re.escape(CONTROL_CHARS))\n",
    "\n",
    "def safe_json_loads(raw: str):\n",
    "    raw = raw.strip()\n",
    "\n",
    "    # Remove markdown fences\n",
    "    raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # Remove ASCII control characters\n",
    "    raw = CONTROL_CHAR_RE.sub('', raw)\n",
    "\n",
    "    # Extract ONLY the first JSON object\n",
    "    match = re.search(r\"\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}\", raw)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in LLM output:\\n\" + raw)\n",
    "\n",
    "    json_str = match.group(0)\n",
    "\n",
    "    # Try strict parse\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Attempt repairs\n",
    "    json_str = json_str.replace(\"'\", '\"')\n",
    "    json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
    "    json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
    "\n",
    "    # Remove control chars again\n",
    "    json_str = CONTROL_CHAR_RE.sub('', json_str)\n",
    "\n",
    "    return json.loads(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d2421d1e-2370-4e12-8d19-c4f62a28625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory: str):\n",
    "    p = Path(directory)\n",
    "    return [\n",
    "        str(f)\n",
    "        for f in p.iterdir()\n",
    "        if f.is_file() and f.suffix.lower() in [\n",
    "            \".pdf\", \".m4a\", \".mp3\", \".wav\", \".png\", \".jpg\", \".jpeg\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "def read_pdf(filepath: str) -> str:\n",
    "    try:\n",
    "        reader = PdfReader(filepath)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            extracted = page.extract_text()\n",
    "            if extracted:\n",
    "                text += extracted\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"[PDF_ERROR] {e}\"\n",
    "\n",
    "\n",
    "def transcribe_audio(filepath: str) -> str:\n",
    "    try:\n",
    "        model = whisper.load_model(\"base\")\n",
    "        result = model.transcribe(filepath)\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"[AUDIO_ERROR] {e}\"\n",
    "\n",
    "\n",
    "def ocr_image(filepath: str) -> str:\n",
    "    try:\n",
    "        img = Image.open(filepath)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"[IMAGE_ERROR] {e}\"\n",
    "\n",
    "\n",
    "def read_template(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def extract_placeholders(template: str):\n",
    "    return re.findall(r\"{(.*?)}\", template)\n",
    "\n",
    "\n",
    "def fill_template(template: str, data: dict) -> str:\n",
    "    content = template\n",
    "    for key in extract_placeholders(template):\n",
    "        value = data.get(key, f\"No {key} found\")\n",
    "        if isinstance(value, list):\n",
    "            value = \"\\n\".join(str(v) for v in value)\n",
    "        content = content.replace(f\"{{{key}}}\", str(value))\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_markdown(filename: str, content: str, out_dir: str) -> str:\n",
    "    out_dir_path = Path(out_dir)\n",
    "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base = filename.lower().replace(\" \", \"-\")\n",
    "    base = re.sub(r\"[^a-z0-9\\-]\", \"-\", base)\n",
    "    final_name = base + \".md\"\n",
    "\n",
    "    out_path = out_dir_path / final_name\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    return str(out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7dcee2de-e597-48a1-842c-f04e8ca3e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_SYSTEM = \"\"\"\n",
    "You interpret the user's instruction and decide whether the agent should process documents.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "\n",
    "{\n",
    "  \"should_process\": true/false,\n",
    "  \"task_type\": \"<string or null>\",\n",
    "  \"task_description\": \"<string or null>\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- If the instruction is a greeting, small talk, or unrelated to document processing → should_process = false.\n",
    "- If the instruction describes a document-processing task (summarize, extract key points, list actions, etc.) → should_process = true.\n",
    "- task_description must be a short natural-language description of what to do.\n",
    "- No extra text. No markdown. Only JSON.\n",
    "\"\"\"\n",
    "\n",
    "def interpret_task(user_instruction: str) -> dict:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": TASK_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": user_instruction}\n",
    "    ]\n",
    "    resp = ollama.chat(model=\"phi3:3.8b\", messages=messages)\n",
    "    raw = resp[\"message\"][\"content\"].strip()\n",
    "    return safe_json_loads(raw)\n",
    "\n",
    "TOOL_CHOICE_SYSTEM = \"\"\"\n",
    "You decide which tool to use for a file.\n",
    "\n",
    "Tools:\n",
    "- read_pdf → .pdf\n",
    "- ocr_image → .png, .jpg, .jpeg\n",
    "- transcribe_audio → .mp3, .wav, m4a\n",
    "\n",
    "Return ONLY:\n",
    "{\"tool\": \"<read_pdf|ocr_image|transcribe_audio>\"}\n",
    "\"\"\"\n",
    "\n",
    "def call_llm_tool_choice(filename: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": TOOL_CHOICE_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": filename}\n",
    "    ]\n",
    "    resp = ollama.chat(model=\"phi3:3.8b\", messages=messages)\n",
    "    raw = resp[\"message\"][\"content\"].strip()\n",
    "    return safe_json_loads(raw)[\"tool\"]\n",
    "\n",
    "CLEAN_TEXT_SYSTEM = \"\"\"\n",
    "Clean the text. Follow these rules:\n",
    "- Output only the cleaned text.\n",
    "- No explanations, notes, lists, commentary, or summaries.\n",
    "- Do not change the meaning or add missing information.\n",
    "Your task:\n",
    "- Fix obvious OCR/transcription errors.\n",
    "- Correct spelling when the intent is clear.\n",
    "- Restore punctuation, spacing, and broken words/lines.\n",
    "- Normalise dates/numbers when unambiguous.\n",
    "- Remove garbage symbols and artefacts.\n",
    "- Preserve the original meaning.\n",
    "Return only the cleaned text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_text(raw_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CLEAN_TEXT_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": raw_text}\n",
    "    ]\n",
    "    resp = ollama.chat(model=\"phi3:3.8b\", messages=messages)\n",
    "    return resp[\"message\"][\"content\"].strip()\n",
    "\n",
    "EXTRACT_SYSTEM = \"\"\"\n",
    "You extract structured information from text and suggest filenames.\n",
    "\n",
    "Return ONLY:\n",
    "{\n",
    "  \"fields\": { ... },\n",
    "  \"suggested_filename\": \"...\"\n",
    "}\n",
    "\n",
    "FILENAME RULES:\n",
    "Use key extracted fields to create a short, descriptive, lowercase, hyphenated filename with no extension. Remove special characters. If useful fields are missing, generate a simple topic‑based name.\n",
    "EXAMPLES:\n",
    "invoice-2024-04-12-abc-ltd\n",
    "meeting-q1-strategy\n",
    "legal-letter-john-doe\n",
    "project-alpha-status\n",
    "\n",
    "GENERAL BEHAVIOUR:\n",
    "- Never add commentary.\n",
    "- Never output markdown.\n",
    "- Never include text outside the JSON.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def call_llm_extract(text: str, fields: list[str], template: str, task_description: str) -> dict:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": EXTRACT_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": json.dumps({\n",
    "            \"fields\": fields,\n",
    "            \"text\": text[:3000],\n",
    "            \"template\": template,\n",
    "            \"task\": task_description\n",
    "        })}\n",
    "    ]\n",
    "    resp = ollama.chat(model=\"llama3.1\", messages=messages)\n",
    "    raw = resp[\"message\"][\"content\"].strip()\n",
    "    return safe_json_loads(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f22bd4b-71f2-40fc-96d8-83b7724d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath: str, template: str, out_dir: str, task_description: str):\n",
    "    suffix = Path(filepath).suffix.lower()\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # LLM decides tool\n",
    "    tool = call_llm_tool_choice(filename)\n",
    "    print(\"Detect \" + suffix + \" file -> Use \" + tool + \" to process the file.\")\n",
    "\n",
    "    # Execute tool\n",
    "    if tool == \"read_pdf\":\n",
    "        raw_text = read_pdf(filepath)\n",
    "    elif tool == \"ocr_image\":\n",
    "        raw_text = ocr_image(filepath)\n",
    "    elif tool == \"transcribe_audio\":\n",
    "        raw_text = transcribe_audio(filepath)\n",
    "\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    # Extract fields + filename\n",
    "    fields = extract_placeholders(template)\n",
    "    print(fields)\n",
    "    \n",
    "    result = call_llm_extract(raw_text, fields, template, task_description)\n",
    "\n",
    "    field_values = result[\"fields\"]\n",
    "    suggested_filename = result[\"suggested_filename\"]\n",
    "    print(field_values)\n",
    "\n",
    "    # Fill template\n",
    "    content = fill_template(template, field_values)\n",
    "\n",
    "    # Save\n",
    "    saved_path = save_markdown(suggested_filename, content, out_dir)\n",
    "    print(f\"Saved: {saved_path}\")\n",
    "\n",
    "\n",
    "def run_agent(documents_dir: str, template_path: str, out_dir: str, user_instruction: str):\n",
    "    task = interpret_task(user_instruction)\n",
    "\n",
    "    if not task[\"should_process\"]:\n",
    "        print(\"User instruction does not request document processing. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Pass task_description into extraction\n",
    "    task_description = task[\"task_description\"]\n",
    "    template = read_template(template_path)\n",
    "    files = list_files(documents_dir)\n",
    "\n",
    "    print(f\"Found {len(files)} files:\")\n",
    "    for f in files:\n",
    "        print(\" -\", f)\n",
    "\n",
    "    # Paralle Processing\n",
    "    with ThreadPoolExecutor(max_workers=4) as ex:\n",
    "        futures = [\n",
    "            ex.submit(process_file, f, template, out_dir, task_description)\n",
    "            for f in files\n",
    "        ]\n",
    "        for fut in futures:\n",
    "            fut.result()  # propagate errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6c02820-ac2a-42aa-ad69-324b7a335542",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    BASE_DIR = Path(__file__).parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_DOCUMENTS_DIR = str(BASE_DIR / \"documents\")\n",
    "DEFAULT_TEMPLATE_PATH = str(BASE_DIR / \"template.md\")\n",
    "DEFAULT_OUTPUT_DIR = str(BASE_DIR / \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2af7d4c4-7498-4bf2-a90f-6b9664b1ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What would you like me to do?\n",
      ">  extract the key info from audio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files:\n",
      " - C:\\Users\\Bunny Liu\\Desktop\\azzurroassociates\\documents\\team_sync.m4a\n",
      "Detect .m4a file -> Use transcribe_audio to process the file.\n",
      "['participants', 'overview', 'key_points', 'next_steps']\n",
      "{'participants': ['Smith', 'Priya Patel', 'Marcus Lee', 'Sarah Chen'], 'overview': 'the team reveal current progress of project Aurora', 'key_points': ['Development of the new analytics dashboard is 80% complete', 'Backend integration schedule to finish by the end of next week', 'Finalized dashboard backend integration by 2020 January'], 'next_steps': ['Deliver update mobile layouts by 70 January', 'Prepare alternative messaging. Angles for reveal', 'API error handling module', 'Validate data accuracy against the latest dataset', 'Coordinate with marketing on launch assets']}\n",
      "Saved: C:\\Users\\Bunny Liu\\Desktop\\azzurroassociates\\output\\project-aurora-status-update.md\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_instruction = input(\"What would you like me to do?\\n> \")\n",
    "\n",
    "    documents_dir = DEFAULT_DOCUMENTS_DIR\n",
    "    template_path = DEFAULT_TEMPLATE_PATH\n",
    "    output_dir = DEFAULT_OUTPUT_DIR\n",
    "\n",
    "    run_agent(documents_dir, template_path, output_dir, user_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5b122-6563-46a9-a9d0-59ddcc693737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
